{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd2b928",
   "metadata": {},
   "source": [
    " 1\tHello, Data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "708bd0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c1a792b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_file_path = 'data/1000 Sales Records.csv'\n",
    "secondary_file_path = 'data/worldcities.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4be6e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Primary Transaction Data (1000 Sales Records.csv) ---\n",
      "First 3 rows:\n",
      "                         Region Country   Item Type Sales Channel  \\\n",
      "0  Middle East and North Africa   Libya   Cosmetics       Offline   \n",
      "1                 North America  Canada  Vegetables        Online   \n",
      "2  Middle East and North Africa   Libya   Baby Food       Offline   \n",
      "\n",
      "  Order Priority  Order Date   Order ID   Ship Date  Units Sold  Unit Price  \\\n",
      "0              M  10/18/2014  686800706  10/31/2014        8446      437.20   \n",
      "1              M   11/7/2011  185941302   12/8/2011        3018      154.06   \n",
      "2              C  10/31/2016  246222341   12/9/2016        1517      255.28   \n",
      "\n",
      "   Unit Cost  Total Revenue  Total Cost  Total Profit  \n",
      "0     263.33     3692591.20  2224085.18    1468506.02  \n",
      "1      90.93      464953.08   274426.74     190526.34  \n",
      "2     159.42      387259.76   241840.14     145419.62  \n",
      "\n",
      "Column names:\n",
      "Index(['Region', 'Country', 'Item Type', 'Sales Channel', 'Order Priority',\n",
      "       'Order Date', 'Order ID', 'Ship Date', 'Units Sold', 'Unit Price',\n",
      "       'Unit Cost', 'Total Revenue', 'Total Cost', 'Total Profit'],\n",
      "      dtype='object')\n",
      "\n",
      "Shape of loaded primary data: (500, 14)\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Primary Transaction Data (1000 Sales Records.csv) ---\")\n",
    "try:\n",
    "    df_sales = pd.read_csv(primary_file_path, nrows=500)\n",
    "    print(\"First 3 rows:\")\n",
    "    print(df_sales.head(3))  \n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df_sales.columns)\n",
    "    print(f\"\\nShape of loaded primary data: {df_sales.shape}\")\n",
    "except (FileNotFoundError, NotADirectoryError) as e:\n",
    "    print(f\"Error: {e}. Make sure '{primary_file_path}' is a valid file in the 'data' subfolder.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4e0eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Secondary Metadata (worldcities.csv) ---\n",
      "First 3 rows:\n",
      "          city   city_ascii      lat       lng        country iso2 iso3  \\\n",
      "0        Tokyo        Tokyo  35.6850  139.7514          Japan   JP  JPN   \n",
      "1     New York     New York  40.6943  -73.9249  United States   US  USA   \n",
      "2  Mexico City  Mexico City  19.4424  -99.1310         Mexico   MX  MEX   \n",
      "\n",
      "         admin_name  capital  population          id  \n",
      "0             Tōkyō  primary  35676000.0  1392685764  \n",
      "1          New York      NaN  19354922.0  1840034016  \n",
      "2  Ciudad de México  primary  19028000.0  1484247881  \n",
      "\n",
      "Column names:\n",
      "Index(['city', 'city_ascii', 'lat', 'lng', 'country', 'iso2', 'iso3',\n",
      "       'admin_name', 'capital', 'population', 'id'],\n",
      "      dtype='object')\n",
      "\n",
      "Shape of loaded secondary data: (15493, 11)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Secondary Metadata (worldcities.csv) ---\")\n",
    "try:\n",
    "    df_cities = pd.read_csv(secondary_file_path)\n",
    "    print(\"First 3 rows:\")\n",
    "    print(df_cities.head(3))\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df_cities.columns)\n",
    "    print(f\"\\nShape of loaded secondary data: {df_cities.shape}\")\n",
    "   \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {secondary_file_path} was not found. Make sure it's in the 'data' subfolder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d798b",
   "metadata": {},
   "source": [
    "2\tPick the Right Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682b690",
   "metadata": {},
   "source": [
    "For storing individual transaction records, a class is the most suitable data structure in Python. This will allow one to encapsulate various data attributes in a transaction-say, date, product, price, etc.-and bundle behaviors related to that data, whether data cleaning or calculating totals via .clean() and .total() methods-so that the behaviors reside in the same place as the data they act upon. This leads to more organized, intuitive, and maintainable codebases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76f3ad",
   "metadata": {},
   "source": [
    "3\tTransaction Class and OO data structure\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f32a2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d44a2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transaction:\n",
    "    def __init__(self, date_val: str, customer_id_val: str, product_val: str,\n",
    "                 price_val: Any, quantity_val: Any, coupon_code_val: Optional[str],\n",
    "                 shipping_city_val: str, order_id_original_val: str,\n",
    "                 region_val: Optional[str] = None, unit_cost_val: Optional[Any] = None,\n",
    "                 sales_channel_val: Optional[str] = None, \n",
    "                 raw_total_revenue: Optional[Any] = None,\n",
    "                 raw_total_cost: Optional[Any] = None,\n",
    "                 raw_total_profit: Optional[Any] = None):\n",
    "\n",
    "        self.date: Optional[datetime] = None\n",
    "        try:\n",
    "            self.date = pd.to_datetime(date_val).to_pydatetime()\n",
    "        except Exception:\n",
    "            self.date = None\n",
    "\n",
    "        self.customer_id: str = str(customer_id_val)\n",
    "        self.product: str = str(product_val)\n",
    "\n",
    "        self.price: float = 0.0\n",
    "        try:\n",
    "            self.price = float(price_val)\n",
    "        except (ValueError, TypeError):\n",
    "            self.price = 0.0\n",
    "\n",
    "        self.quantity: int = 0\n",
    "        try:\n",
    "            self.quantity = int(quantity_val)\n",
    "        except (ValueError, TypeError):\n",
    "            self.quantity = 0\n",
    "\n",
    "        self.coupon_code: Optional[str] = coupon_code_val\n",
    "        self.shipping_city: str = str(shipping_city_val)\n",
    "\n",
    "        self.order_id_original: str = str(order_id_original_val)\n",
    "        self.region_original: Optional[str] = str(region_val) if region_val else None\n",
    "        self.sales_channel_original: Optional[str] = str(sales_channel_val) if sales_channel_val else None # Attribute to store it\n",
    "\n",
    "        self.unit_cost_original: float = 0.0\n",
    "        try:\n",
    "            self.unit_cost_original = float(unit_cost_val) if unit_cost_val is not None else 0.0\n",
    "        except (ValueError, TypeError):\n",
    "            self.unit_cost_original = 0.0\n",
    "\n",
    "        self.total_revenue_csv: float = 0.0\n",
    "        try:\n",
    "            self.total_revenue_csv = float(raw_total_revenue) if raw_total_revenue is not None else 0.0\n",
    "        except (ValueError, TypeError):\n",
    "            self.total_revenue_csv = 0.0\n",
    "\n",
    "        self.discount_rate: float = 0.0\n",
    "        self.discount_amount: float = 0.0\n",
    "        self.subtotal: float = self.price * self.quantity\n",
    "        self.final_total: float = self.subtotal\n",
    "        self.days_since_purchase: Optional[int] = None\n",
    "\n",
    "    def clean(self):\n",
    "        self.product = self.product.strip().title()\n",
    "        if self.price < 0:\n",
    "            self.price = 0.0\n",
    "        if self.quantity < 0:\n",
    "            self.quantity = 0\n",
    "        self.subtotal = self.price * self.quantity \n",
    "        self.final_total = self.subtotal * (1 - self.discount_rate) \n",
    "\n",
    "    def apply_discount(self, discount_rate: float):\n",
    "        self.discount_rate = discount_rate\n",
    "        self.subtotal = self.price * self.quantity \n",
    "        self.discount_amount = self.subtotal * self.discount_rate\n",
    "        self.final_total = self.subtotal * (1 - self.discount_rate)\n",
    "\n",
    "    def calculate_total(self):\n",
    "        self.subtotal = self.price * self.quantity\n",
    "        self.final_total = self.subtotal * (1 - self.discount_rate)\n",
    "        return self.final_total\n",
    "\n",
    "    def __repr__(self):\n",
    "        date_str = self.date.strftime('%Y-%m-%d') if self.date else \"N/A\"\n",
    "        return (f\"Transaction(Date: {date_str}, CustID: {self.customer_id}, Prod: {self.product}, \"\n",
    "                f\"Price: {self.price:.2f}, Qty: {self.quantity}, City: {self.shipping_city}, \"\n",
    "                f\"Coupon: {self.coupon_code}, Final Total: {self.final_total:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734edc9",
   "metadata": {},
   "source": [
    "4\tBulk Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a1d81192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading All Transactions (First 500 from primary CSV) ---\n",
      "Successfully loaded and created 500 Transaction objects.\n",
      "\n",
      "First 5 loaded Transaction objects:\n",
      "Transaction(Date: 2014-10-18, CustID: 686800706, Prod: Cosmetics, Price: 437.20, Qty: 8446, City: Tripoli, Coupon: FREESHIP15, Final Total: 3692591.20)\n",
      "Transaction(Date: 2011-11-07, CustID: 185941302, Prod: Vegetables, Price: 154.06, Qty: 3018, City: Ottawa, Coupon: None, Final Total: 464953.08)\n",
      "Transaction(Date: 2016-10-31, CustID: 246222341, Prod: Baby Food, Price: 255.28, Qty: 1517, City: Tripoli, Coupon: SAVE20, Final Total: 387259.76)\n",
      "Transaction(Date: 2010-04-10, CustID: 161442649, Prod: Cereal, Price: 205.70, Qty: 3322, City: Tokyo, Coupon: FREESHIP15, Final Total: 683335.40)\n",
      "Transaction(Date: 2011-08-16, CustID: 645713555, Prod: Fruits, Price: 9.33, Qty: 9845, City: N’Djamena, Coupon: NOCODE, Final Total: 91853.85)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "from typing import List, Dict, Any, Optional \n",
    "\n",
    "def create_city_lookup_dict(cities_df: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Creates a lookup dictionary for country to its primary, admin, or first listed city.\n",
    "    \"\"\"\n",
    "    if cities_df.empty or not all(col in cities_df.columns for col in ['country', 'city', 'population', 'capital']):\n",
    "        return {}\n",
    "\n",
    "    cities_df_sorted = cities_df.copy()\n",
    "\n",
    "    cities_df_sorted['population'] = pd.to_numeric(cities_df_sorted['population'], errors='coerce').fillna(0)\n",
    "    \n",
    "    capital_priority = {'primary': 1, 'admin': 2}\n",
    "    cities_df_sorted['capital_priority'] = cities_df_sorted['capital'].map(capital_priority).fillna(3)\n",
    "\n",
    "    cities_df_sorted = cities_df_sorted.sort_values(\n",
    "        by=['country', 'capital_priority', 'population'],\n",
    "        ascending=[True, True, False] \n",
    "    )\n",
    "    \n",
    "    city_lookup = cities_df_sorted.groupby('country')['city'].first().to_dict()\n",
    "    return city_lookup\n",
    "\n",
    "def load_transactions(primary_filepath_arg: str, cities_df_arg: pd.DataFrame) -> List[Transaction]:\n",
    "    \"\"\"\n",
    "    Loads transaction data from a CSV file path, enriches it with synthetic coupon codes\n",
    "    and derived shipping cities, and returns a list of Transaction objects.\n",
    "    \"\"\"\n",
    "    loaded_transactions_list: List[Transaction] = []\n",
    "    \n",
    "    try:\n",
    "        sales_df = pd.read_csv(primary_filepath_arg, nrows=500)\n",
    "\n",
    "        if sales_df.empty:\n",
    "            print(\"Sales DataFrame is empty after loading.\")\n",
    "            return loaded_transactions_list\n",
    "\n",
    "        coupon_options = [None, \"SAVE10\", \"SAVE20\", \"FREESHIP15\", \"NOCODE\"]\n",
    "        weights = [0.5, 0.15, 0.15, 0.1, 0.1] \n",
    "        sales_df['coupon_code_synthetic'] = random.choices(coupon_options, weights=weights, k=len(sales_df))\n",
    "\n",
    "\n",
    "        city_lookup = create_city_lookup_dict(cities_df_arg)\n",
    "        if not city_lookup:\n",
    "            print(\"City lookup dictionary is empty. Shipping cities may not be resolved correctly.\")\n",
    "\n",
    "       \n",
    "        for _, row in sales_df.iterrows():\n",
    "            transaction_country = str(row.get('Country', \"Unknown Country\")).strip()\n",
    "            \n",
    "            resolved_shipping_city = city_lookup.get(transaction_country, transaction_country)\n",
    "\n",
    "           \n",
    "            transaction_obj = Transaction(\n",
    "                date_val=row['Order Date'],\n",
    "                customer_id_val=str(row['Order ID']),               \n",
    "                product_val=str(row['Item Type']),\n",
    "                price_val=row['Unit Price'],\n",
    "                quantity_val=row['Units Sold'],\n",
    "                coupon_code_val=row['coupon_code_synthetic'],       \n",
    "                shipping_city_val=resolved_shipping_city,           \n",
    "                order_id_original_val=str(row['Order ID']),        \n",
    "                region_val=str(row.get('Region')),               \n",
    "                unit_cost_val=row.get('Unit Cost'),                 \n",
    "                sales_channel_val=str(row.get('Sales Channel')),  \n",
    "                raw_total_revenue=row.get('Total Revenue'),         \n",
    "                raw_total_cost=row.get('Total Cost'),               \n",
    "                raw_total_profit=row.get('Total Profit')            \n",
    "            )\n",
    "            loaded_transactions_list.append(transaction_obj)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {primary_filepath_arg} was not found.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: A required column is missing from the CSV - {e}. Check CSV headers and Transaction class arguments.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during transaction loading: {e}\")\n",
    "        \n",
    "    return loaded_transactions_list\n",
    "\n",
    "\n",
    "if 'df_cities' in globals() and 'primary_file_path' in globals():\n",
    "    print(\"\\n--- Loading All Transactions (First 500 from primary CSV) ---\")\n",
    "    all_transactions = load_transactions(primary_file_path, df_cities) \n",
    "    \n",
    "    if all_transactions:\n",
    "        print(f\"Successfully loaded and created {len(all_transactions)} Transaction objects.\")\n",
    "        print(\"\\nFirst 5 loaded Transaction objects:\")\n",
    "        for i in range(min(5, len(all_transactions))):\n",
    "            print(all_transactions[i])\n",
    "    else:\n",
    "        print(\"No transactions were loaded. Check for errors above or if the primary CSV is empty/incorrectly formatted.\")\n",
    "else:\n",
    "    print(\"\\n'df_cities' (from worldcities.csv) or 'primary_file_path' (for sales data) is not defined.\")\n",
    "    print(\"Please ensure Step 1 (Hello, Data!) has been executed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd65721",
   "metadata": {},
   "source": [
    "5\tQuick Profiling\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5811d4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Price Profiling ---\n",
      "Minimum Price: 9.33\n",
      "Maximum Price: 668.27\n",
      "Mean Price: 274.30\n",
      "\n",
      "--- Shipping City Profiling ---\n",
      "Number of Unique Shipping Cities/Countries (Proxy): 171\n"
     ]
    }
   ],
   "source": [
    "if 'all_transactions' in globals() and all_transactions:\n",
    "    prices = [t.price for t in all_transactions if t.price is not None]\n",
    "    shipping_cities = {t.shipping_city for t in all_transactions if t.shipping_city is not None}\n",
    "\n",
    "    if prices:\n",
    "        min_price = min(prices)\n",
    "        max_price = max(prices)\n",
    "        mean_price = sum(prices) / len(prices)\n",
    "        print(f\"--- Price Profiling ---\")\n",
    "        print(f\"Minimum Price: {min_price:.2f}\")\n",
    "        print(f\"Maximum Price: {max_price:.2f}\")\n",
    "        print(f\"Mean Price: {mean_price:.2f}\")\n",
    "    else:\n",
    "        print(\"No valid prices found for profiling.\")\n",
    "\n",
    "    print(f\"\\n--- Shipping City Profiling ---\")\n",
    "    print(f\"Number of Unique Shipping Cities/Countries (Proxy): {len(shipping_cities)}\")\n",
    "else:\n",
    "    print(\"Transaction list not loaded. Please run Step 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b2e4b",
   "metadata": {},
   "source": [
    "6\tSpot the Grime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eb6955b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Identifying Potential Data Grime ---\n",
      "Simulated: Whitespace/casing issues in 'Item Type' for first two records.\n",
      "Simulated: NaN in 'Unit Cost' for the third record.\n",
      "Simulated: Negative 'Unit Price' for the fourth record.\n",
      "\n",
      "--- Grime Check 1: 'Item Type' Formatting ---\n",
      "Unique 'Item Type' values (potential whitespace/casing issues): ['  cosmetics ' 'Vegetables  ' 'Baby Food' 'Cereal' 'Fruits' 'Clothes'\n",
      " 'Vegetables' 'Snacks' 'Household' 'Cosmetics']\n",
      "Number of 'Item Type' entries with leading/trailing whitespace or non-title case: 2\n",
      "\n",
      "--- Grime Check 2: Missing 'Unit Cost' ---\n",
      "Number of rows with missing 'Unit Cost': 1\n",
      "\n",
      "--- Grime Check 3: Negative 'Unit Price' ---\n",
      "Number of rows with negative 'Unit Price': 1\n",
      "Example rows with negative 'Unit Price':\n",
      "  Item Type  Unit Price\n",
      "3    Cereal       -10.0\n",
      "\n",
      "--- Additional Original Grime Checks ---\n",
      "4. Unique 'Order Priority' values found: ['M' 'C' 'H' 'L']\n",
      "5. Number of rows where 'Total Revenue' != 'Units Sold' * 'Cleaned Unit Price': 1\n",
      "Example discrepancies (first 2):\n",
      "   Units Sold  Unit Price  Total Revenue  Calculated Revenue\n",
      "3        3322       -10.0       683335.4                 0.0\n"
     ]
    }
   ],
   "source": [
    "if 'df_sales' in globals() and not df_sales.empty:\n",
    "    print(\"--- Identifying Potential Data Grime ---\")\n",
    "    \n",
    "    df_sales_grime_check = df_sales.copy()\n",
    "\n",
    "\n",
    "    if len(df_sales_grime_check) > 2:\n",
    "        df_sales_grime_check.loc[0, 'Item Type'] = '  cosmetics '  \n",
    "        df_sales_grime_check.loc[1, 'Item Type'] = 'Vegetables  ' \n",
    "    print(\"Simulated: Whitespace/casing issues in 'Item Type' for first two records.\")\n",
    "\n",
    "    if len(df_sales_grime_check) > 3:\n",
    "        df_sales_grime_check.loc[2, 'Unit Cost'] = np.nan\n",
    "    print(\"Simulated: NaN in 'Unit Cost' for the third record.\")\n",
    "\n",
    "\n",
    "    if len(df_sales_grime_check) > 4:\n",
    "        df_sales_grime_check.loc[3, 'Unit Price'] = -10.0\n",
    "    print(\"Simulated: Negative 'Unit Price' for the fourth record.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Grime Check 1: 'Item Type' Formatting ---\")\n",
    "    original_item_types = df_sales_grime_check['Item Type'].unique()\n",
    "    print(f\"Unique 'Item Type' values (potential whitespace/casing issues): {original_item_types[:10]}\") \n",
    "    \n",
    "\n",
    "    changed_item_types_count = 0\n",
    "    if 'Item Type' in df_sales_grime_check.columns:\n",
    "        for item in df_sales_grime_check['Item Type'].dropna().astype(str): \n",
    "            if item != item.strip().title():\n",
    "                changed_item_types_count += 1\n",
    "    print(f\"Number of 'Item Type' entries with leading/trailing whitespace or non-title case: {changed_item_types_count}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n--- Grime Check 2: Missing 'Unit Cost' ---\")\n",
    "    if 'Unit Cost' in df_sales_grime_check.columns:\n",
    "        missing_unit_costs = df_sales_grime_check['Unit Cost'].isnull().sum()\n",
    "        print(f\"Number of rows with missing 'Unit Cost': {missing_unit_costs}\")\n",
    "    else:\n",
    "        print(\"'Unit Cost' column not found.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Grime Check 3: Negative 'Unit Price' ---\")\n",
    "    if 'Unit Price' in df_sales_grime_check.columns:\n",
    "        negative_prices = df_sales_grime_check[df_sales_grime_check['Unit Price'] < 0]\n",
    "        print(f\"Number of rows with negative 'Unit Price': {len(negative_prices)}\")\n",
    "        if not negative_prices.empty:\n",
    "            print(\"Example rows with negative 'Unit Price':\")\n",
    "            print(negative_prices[['Item Type', 'Unit Price']].head())\n",
    "    else:\n",
    "        print(\"'Unit Price' column not found.\")\n",
    "        \n",
    "    \n",
    "    print(\"\\n--- Additional Original Grime Checks ---\")\n",
    "\n",
    "    if 'Order Priority' in df_sales_grime_check.columns:\n",
    "        unique_priorities = df_sales_grime_check['Order Priority'].unique()\n",
    "        print(f\"4. Unique 'Order Priority' values found: {unique_priorities}\")\n",
    "    else:\n",
    "        print(\"Column 'Order Priority' not found.\")\n",
    "\n",
    "    \n",
    "    if all(col in df_sales_grime_check.columns for col in ['Units Sold', 'Unit Price', 'Total Revenue']):\n",
    "        \n",
    "        df_sales_grime_check['Temp Unit Price for Calc'] = df_sales_grime_check['Unit Price'].apply(lambda x: max(0, x))\n",
    "        df_sales_grime_check['Calculated Revenue'] = df_sales_grime_check['Units Sold'] * df_sales_grime_check['Temp Unit Price for Calc']\n",
    "        \n",
    "        revenue_discrepancies = df_sales_grime_check[\n",
    "            ~np.isclose(df_sales_grime_check['Total Revenue'], df_sales_grime_check['Calculated Revenue'])\n",
    "        ]\n",
    "        print(f\"5. Number of rows where 'Total Revenue' != 'Units Sold' * 'Cleaned Unit Price': {len(revenue_discrepancies)}\")\n",
    "        if not revenue_discrepancies.empty:\n",
    "            print(\"Example discrepancies (first 2):\")\n",
    "            print(revenue_discrepancies[['Units Sold', 'Unit Price', 'Total Revenue', 'Calculated Revenue']].head(2))\n",
    "        df_sales_grime_check.drop(columns=['Temp Unit Price for Calc', 'Calculated Revenue'], inplace=True, errors='ignore')\n",
    "\n",
    "    else:\n",
    "        print(\"One or more financial columns for consistency check are missing.\")\n",
    "\n",
    "else:\n",
    "    print(\"Sales DataFrame (df_sales) not loaded. Please run Step 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a10a03",
   "metadata": {},
   "source": [
    "7\tCleaning Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0995a8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying Cleaning Rules to all Transaction objects ---\n",
      "\n",
      "Example 1: Product Name Cleaning\n",
      "  Transaction 0 Product (Before clean): '  fruits  '\n",
      "  Transaction 0 Product (After clean): 'Fruits'\n",
      "\n",
      "Example 2: Negative Price Cleaning\n",
      "  Transaction 1 (Before clean): Price=-50.0, FinalTotal=-100.0\n",
      "  Transaction 1 (After clean): Price=0.0, FinalTotal=0.0\n",
      "\n",
      "Applying .clean() to all remaining transactions...\n",
      "Cleaning applied to all transactions.\n",
      "\n",
      "Number of transactions with negative prices after all have been cleaned: 0\n",
      "\n",
      "First 3 product names after all transactions cleaned:\n",
      "  Transaction 0 Product: 'Fruits'\n",
      "  Transaction 1 Product: 'Vegetables'\n",
      "  Transaction 2 Product: 'Baby Food'\n"
     ]
    }
   ],
   "source": [
    "if 'all_transactions' in globals() and all_transactions:\n",
    "    print(\"--- Applying Cleaning Rules to all Transaction objects ---\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(all_transactions) > 0:\n",
    "    \n",
    "        print(f\"\\nExample 1: Product Name Cleaning\")\n",
    "       \n",
    "        all_transactions[0].product = \"  fruits  \" \n",
    "        print(f\"  Transaction 0 Product (Before clean): '{all_transactions[0].product}'\")\n",
    "        all_transactions[0].clean() \n",
    "        print(f\"  Transaction 0 Product (After clean): '{all_transactions[0].product}'\")\n",
    "\n",
    "        \n",
    "        if len(all_transactions) > 1:\n",
    "            print(f\"\\nExample 2: Negative Price Cleaning\")\n",
    "            all_transactions[1].price = -50.0 \n",
    "            all_transactions[1].quantity = 2  \n",
    "            all_transactions[1].calculate_total() \n",
    "            print(f\"  Transaction 1 (Before clean): Price={all_transactions[1].price}, FinalTotal={all_transactions[1].final_total}\")\n",
    "            all_transactions[1].clean() \n",
    "            print(f\"  Transaction 1 (After clean): Price={all_transactions[1].price}, FinalTotal={all_transactions[1].final_total}\")\n",
    "    \n",
    "   \n",
    "    print(\"\\nApplying .clean() to all remaining transactions...\")\n",
    "    for t_idx in range(len(all_transactions)): \n",
    "        if t_idx > 1: \n",
    "             all_transactions[t_idx].clean()\n",
    "    print(\"Cleaning applied to all transactions.\")\n",
    "\n",
    "   \n",
    "    negative_prices_after_all_cleaned = sum(1 for t in all_transactions if t.price < 0)\n",
    "    print(f\"\\nNumber of transactions with negative prices after all have been cleaned: {negative_prices_after_all_cleaned}\")\n",
    "\n",
    "   \n",
    "    print(\"\\nFirst 3 product names after all transactions cleaned:\")\n",
    "    for i in range(min(3, len(all_transactions))):\n",
    "        print(f\"  Transaction {i} Product: '{all_transactions[i].product}'\")\n",
    "else:\n",
    "    print(\"Transaction list (all_transactions) not loaded or empty. Please run Step 4.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13518212",
   "metadata": {},
   "source": [
    "8\tTransformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7a92f974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying Coupon Code Transformations & Updating Totals for ALL Transactions ---\n",
      "\n",
      "Sample of first 3 transactions (state before this systematic discount application):\n",
      "  ID: 686800706, Coupon: FREESHIP15, Subtotal: 3692591.20, Current Discount Rate: 0.00, Final Total: 3692591.20\n",
      "  ID: 185941302, Coupon: None, Subtotal: 0.00, Current Discount Rate: 0.00, Final Total: 0.00\n",
      "  ID: 246222341, Coupon: SAVE20, Subtotal: 387259.76, Current Discount Rate: 0.00, Final Total: 387259.76\n",
      "\n",
      "Sample of first 3 transactions (state after this systematic discount application):\n",
      "  ID: 686800706, Coupon: FREESHIP15, Subtotal: 3692591.20, New Discount Rate: 0.00, Discount Amount: 0.00, Final Total: 3692591.20\n",
      "  ID: 185941302, Coupon: None, Subtotal: 0.00, New Discount Rate: 0.00, Discount Amount: 0.00, Final Total: 0.00\n",
      "  ID: 246222341, Coupon: SAVE20, Subtotal: 387259.76, New Discount Rate: 0.20, Discount Amount: 77451.95, Final Total: 309807.81\n",
      "\n",
      "--- Transformation Summary for All Transactions ---\n",
      "Total transactions processed: 500\n",
      "Number of transactions that received a discount: 135\n",
      "Sum of all final_totals BEFORE coupon transformation: 710727274.13\n",
      "Sum of all final_totals AFTER coupon transformation: 681590415.36\n"
     ]
    }
   ],
   "source": [
    "def get_discount_rate_from_coupon(coupon_code: Optional[str]) -> float:\n",
    "    \"\"\"Maps a coupon code string to a numeric discount rate.\"\"\"\n",
    "    if coupon_code == \"SAVE10\":\n",
    "        return 0.10\n",
    "    elif coupon_code == \"SAVE20\":\n",
    "        return 0.20\n",
    "    elif coupon_code == \"SUMMER25\":\n",
    "        return 0.25\n",
    "    elif coupon_code == \"FREESHIP\": \n",
    "        return 0.15 \n",
    "    else: \n",
    "        return 0.0\n",
    "\n",
    "if 'all_transactions' in globals() and all_transactions:\n",
    "    print(\"--- Applying Coupon Code Transformations & Updating Totals for ALL Transactions ---\")\n",
    "    \n",
    "    sum_final_total_before_coupon_logic = 0\n",
    "    for t_before in all_transactions:\n",
    "        t_before.calculate_total() \n",
    "        sum_final_total_before_coupon_logic += t_before.final_total\n",
    "\n",
    "    print(\"\\nSample of first 3 transactions (state before this systematic discount application):\")\n",
    "    for i in range(min(3, len(all_transactions))):\n",
    "        t = all_transactions[i]\n",
    "        print(f\"  ID: {t.order_id_original}, Coupon: {t.coupon_code}, Subtotal: {t.subtotal:.2f}, Current Discount Rate: {t.discount_rate:.2f}, Final Total: {t.final_total:.2f}\")\n",
    "\n",
    "    \n",
    "    transactions_with_discount_applied = 0\n",
    "    for t in all_transactions:\n",
    "        new_discount_rate = get_discount_rate_from_coupon(t.coupon_code)\n",
    "        t.apply_discount(new_discount_rate) \n",
    "        if new_discount_rate > 0:\n",
    "            transactions_with_discount_applied += 1\n",
    "    \n",
    "    sum_final_total_after_coupon_logic = sum(t.final_total for t in all_transactions)\n",
    "\n",
    "    print(\"\\nSample of first 3 transactions (state after this systematic discount application):\")\n",
    "    for i in range(min(3, len(all_transactions))):\n",
    "        t = all_transactions[i] \n",
    "        print(f\"  ID: {t.order_id_original}, Coupon: {t.coupon_code}, Subtotal: {t.subtotal:.2f}, New Discount Rate: {t.discount_rate:.2f}, Discount Amount: {t.discount_amount:.2f}, Final Total: {t.final_total:.2f}\")\n",
    "\n",
    "    print(\"\\n--- Transformation Summary for All Transactions ---\")\n",
    "    print(f\"Total transactions processed: {len(all_transactions)}\")\n",
    "    print(f\"Number of transactions that received a discount: {transactions_with_discount_applied}\")\n",
    "    print(f\"Sum of all final_totals BEFORE coupon transformation: {sum_final_total_before_coupon_logic:.2f}\")\n",
    "    print(f\"Sum of all final_totals AFTER coupon transformation: {sum_final_total_after_coupon_logic:.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Transaction list (all_transactions) not loaded or empty. Please run Step 4 & 7.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b3e4b",
   "metadata": {},
   "source": [
    "9\tFeature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "054a086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature Engineering: Adding 'days_since_purchase' ---\n",
      "Reference date for 'days_since_purchase' calculation: 2017-07-26\n",
      "\n",
      "First 3 transactions with engineered 'days_since_purchase' and financial details:\n",
      "  Transaction OrderID: 686800706, Date: 2014-10-18, Subtotal: 3692591.20, DiscountAmt: 0.00, FinalTotal: 3692591.20, DaysSincePurchase: 1012\n",
      "  Transaction OrderID: 185941302, Date: 2011-11-07, Subtotal: 0.00, DiscountAmt: 0.00, FinalTotal: 0.00, DaysSincePurchase: 2088\n",
      "  Transaction OrderID: 246222341, Date: 2016-10-31, Subtotal: 387259.76, DiscountAmt: 77451.95, FinalTotal: 309807.81, DaysSincePurchase: 268\n"
     ]
    }
   ],
   "source": [
    "if 'all_transactions' in globals() and all_transactions:\n",
    "    print(\"--- Feature Engineering: Adding 'days_since_purchase' ---\")\n",
    "    \n",
    "    valid_dates = [t.date for t in all_transactions if t.date is not None]\n",
    "    if not valid_dates:\n",
    "        print(\"No valid dates in transactions to determine a reference date. Skipping 'days_since_purchase'.\")\n",
    "        for t in all_transactions:\n",
    "            t.days_since_purchase = None\n",
    "    else:\n",
    "        current_processing_date = max(valid_dates) \n",
    "        print(f\"Reference date for 'days_since_purchase' calculation: {current_processing_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        for t in all_transactions:\n",
    "            if t.date: \n",
    "                t.days_since_purchase = (current_processing_date - t.date).days\n",
    "            else:\n",
    "                t.days_since_purchase = None \n",
    "            \n",
    "            t.calculate_total() \n",
    "\n",
    "        print(\"\\nFirst 3 transactions with engineered 'days_since_purchase' and financial details:\")\n",
    "        for i in range(min(3, len(all_transactions))):\n",
    "            t = all_transactions[i]\n",
    "            date_str = t.date.strftime('%Y-%m-%d') if t.date else 'N/A'\n",
    "            print(f\"  Transaction OrderID: {t.order_id_original}, Date: {date_str}, \"\n",
    "                  f\"Subtotal: {t.subtotal:.2f}, DiscountAmt: {t.discount_amount:.2f}, FinalTotal: {t.final_total:.2f}, \"\n",
    "                  f\"DaysSincePurchase: {t.days_since_purchase}\")\n",
    "else:\n",
    "    print(\"Transaction list (all_transactions) not loaded or empty. Please run previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d9448",
   "metadata": {},
   "source": [
    "10\tMini-Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "99690ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Revenue Per Shipping City/Country (Proxy) ---\n",
      "City/Country                   |   Total Revenue\n",
      "------------------------------------------------\n",
      "Port Moresby                   |     15629197.78\n",
      "San José                       |     15578320.59\n",
      "Lisbon                         |     12486365.66\n",
      "Czech Republic                 |     11925961.58\n",
      "Havana                         |     11889494.51\n",
      "United States of America       |     11837837.23\n",
      "Nuku‘alofa                     |     11821702.00\n",
      "Cape Town                      |     11276161.63\n",
      "Tbilisi                        |     11210683.14\n",
      "N’Djamena                      |     11141002.35\n",
      "Luxembourg                     |     10875934.86\n",
      "Vienna                         |      9889550.02\n",
      "Mbabane                        |      9840650.51\n",
      "Ankara                         |      9787768.05\n",
      "Hanoi                          |      9437912.38\n",
      "Maputo                         |      8721343.91\n",
      "Sisimiut                       |      8551967.53\n",
      "Bangui                         |      8227617.55\n",
      "The Bahamas                    |      7980934.29\n",
      "Tashkent                       |      7962854.52\n",
      "Accra                          |      7930984.13\n",
      "Berlin                         |      7413459.05\n",
      "Chisinau                       |      7261959.29\n",
      "Malabo                         |      7091349.15\n",
      "Beirut                         |      6843671.85\n",
      "Vaduz                          |      6777517.84\n",
      "Tokyo                          |      6766832.07\n",
      "Vilnius                        |      6746156.81\n",
      "Wellington                     |      6533529.54\n",
      "Budapest                       |      6512861.89\n",
      "Sanaa                          |      6367993.03\n",
      "Yerevan                        |      6352937.06\n",
      "Khartoum                       |      6251548.06\n",
      "Warsaw                         |      6222100.22\n",
      "Conakry                        |      6124648.59\n",
      "Tallinn                        |      6099980.44\n",
      "Lilongwe                       |      5967567.34\n",
      "Port-Vila                      |      5956952.25\n",
      "Minsk                          |      5923061.68\n",
      "Rabat                          |      5907919.53\n",
      "Tunis                          |      5864893.44\n",
      "Kyiv                           |      5823124.60\n",
      "Tripoli                        |      5805778.22\n",
      "Algiers                        |      5644388.66\n",
      "Tegucigalpa                    |      5595560.94\n",
      "Harare                         |      5486179.23\n",
      "Bucharest                      |      5474758.22\n",
      "Stockholm                      |      5451741.78\n",
      "Antigua and Barbuda            |      5247076.83\n",
      "New Delhi                      |      5216723.62\n",
      "Rome                           |      5171516.03\n",
      "Dakar                          |      5161566.69\n",
      "Santo Domingo                  |      5144524.54\n",
      "Belgrade                       |      5136153.58\n",
      "Panama City                    |      5082173.28\n",
      "Kuala Lumpur                   |      5056817.69\n",
      "Damascus                       |      5002000.95\n",
      "Juba                           |      4962183.45\n",
      "Niamey                         |      4948393.34\n",
      "Nicosia                        |      4890570.39\n",
      "Tirana                         |      4843546.14\n",
      "Belmopan                       |      4824916.47\n",
      "Libreville                     |      4801643.21\n",
      "Doha                           |      4735397.77\n",
      "Brussels                       |      4713145.32\n",
      "East Timor                     |      4657835.40\n",
      "Kigali                         |      4635163.97\n",
      "Dublin                         |      4529521.54\n",
      "Bujumbura                      |      4475883.80\n",
      "Bissau                         |      4268426.90\n",
      "Baghdad                        |      4136051.04\n",
      "Athens                         |      4131876.13\n",
      "Maseru                         |      4033724.07\n",
      "Sofia                          |      3963937.88\n",
      "Thimphu                        |      3877027.71\n",
      "Nouakchott                     |      3837977.93\n",
      "Tehran                         |      3805222.43\n",
      "London                         |      3772688.67\n",
      "Luanda                         |      3736167.30\n",
      "Ashgabat                       |      3486232.80\n",
      "Republic of the Congo          |      3473096.28\n",
      "Taipei                         |      3396878.14\n",
      "Valletta                       |      3357958.32\n",
      "Bamako                         |      3296538.92\n",
      "Baku                           |      3250840.32\n",
      "Jerusalem                      |      3224983.69\n",
      "North Korea                    |      3162233.01\n",
      "Amman                          |      3140104.38\n",
      "Port Louis                     |      3113270.29\n",
      "Helsinki                       |      3043469.52\n",
      "Madrid                         |      2977142.85\n",
      "Dhaka                          |      2958479.76\n",
      "Dushanbe                       |      2929287.79\n",
      "Djibouti                       |      2901920.49\n",
      "Manama                         |      2862198.19\n",
      "Ulaanbaatar                    |      2793093.42\n",
      "Democratic Republic of the Congo |      2712547.79\n",
      "Saint Kitts and Nevis          |      2633641.20\n",
      "Guatemala City                 |      2550246.14\n",
      "Bangkok                        |      2545041.31\n",
      "Saint George’s                 |      2490893.32\n",
      "Bern                           |      2486892.76\n",
      "Myanmar                        |      2478319.86\n",
      "Mogadishu                      |      2373912.10\n",
      "Ljubljana                      |      2349280.17\n",
      "Bandar Seri Begawan            |      2247229.84\n",
      "Ouagadougou                    |      2224160.73\n",
      "Honiara                        |      2166377.67\n",
      "Andorra la Vella               |      2154058.43\n",
      "Moscow                         |      2054579.66\n",
      "Cotonou                        |      2036078.19\n",
      "Ngerulmud                      |      1916829.19\n",
      "Tarawa                         |      1831763.01\n",
      "Kampala                        |      1792630.57\n",
      "Podgorica                      |      1788464.19\n",
      "Lomé                           |      1734332.62\n",
      "Castries                       |      1721549.67\n",
      "Singapore                      |      1717389.30\n",
      "Manila                         |      1707940.25\n",
      "Managua                        |      1668084.59\n",
      "Skopje                         |      1637644.19\n",
      "Monrovia                       |      1617852.91\n",
      "Majuro                         |      1583204.05\n",
      "San Salvador                   |      1582362.81\n",
      "Suva                           |      1573089.60\n",
      "Reykjavík                      |      1567469.80\n",
      "Cape Verde                     |      1484547.59\n",
      "Kabul                          |      1476984.10\n",
      "Asmara                         |      1474455.62\n",
      "Sao Tome and Principe          |      1381689.75\n",
      "Bratislava                     |      1331968.33\n",
      "Beijing                        |      1268799.08\n",
      "Kathmandu                      |      1153046.66\n",
      "Nauru                          |      1151598.50\n",
      "Riyadh                         |      1143433.32\n",
      "Oslo                           |      1117593.43\n",
      "South Korea                    |      1078801.46\n",
      "Canberra                       |      1065324.90\n",
      "Dar es Salaam                  |      1027516.93\n",
      "Islamabad                      |       999068.29\n",
      "Gaborone                       |       994120.16\n",
      "Nairobi                        |       983572.73\n",
      "Moroni                         |       831622.03\n",
      "Mexico City                    |       790859.36\n",
      "Trinidad and Tobago            |       786224.23\n",
      "Victoria                       |       762803.62\n",
      "Ottawa                         |       750464.48\n",
      "Kingston                       |       713942.03\n",
      "Muscat                         |       713921.82\n",
      "Zagreb                         |       708057.05\n",
      "Vatican City                   |       678332.84\n",
      "Abu Dhabi                      |       665128.18\n",
      "Cote d'Ivoire                  |       588129.08\n",
      "Monaco                         |       538470.51\n",
      "Male                           |       535394.23\n",
      "Jakarta                        |       486734.76\n",
      "Vientiane                      |       458966.01\n",
      "Federated States of Micronesia |       458738.70\n",
      "Cairo                          |       424867.30\n",
      "Windhoek                       |       344052.60\n",
      "Funafuti                       |       319044.78\n",
      "Nur-Sultan                     |       312885.30\n",
      "Copenhagen                     |       260832.65\n",
      "Addis Ababa                    |       249226.07\n",
      "Phnom Penh                     |       241228.98\n",
      "Abuja                          |       131748.76\n",
      "Yaoundé                        |        95209.92\n",
      "Bridgetown                     |        81981.23\n",
      "Antananarivo                   |        71747.70\n",
      "Apia                           |        61109.38\n",
      "Bishkek                        |        12007.71\n"
     ]
    }
   ],
   "source": [
    "if 'all_transactions' in globals() and all_transactions:\n",
    "    revenue_per_city: Dict[str, float] = {}\n",
    "    for t in all_transactions:\n",
    "        current_final_total = t.final_total if isinstance(t.final_total, (int, float)) and pd.notna(t.final_total) else 0.0\n",
    "        \n",
    "        city_key = t.shipping_city if pd.notna(t.shipping_city) else \"Unknown City\"\n",
    "        revenue_per_city[city_key] = revenue_per_city.get(city_key, 0.0) + current_final_total\n",
    "\n",
    "    print(\"--- Revenue Per Shipping City/Country (Proxy) ---\")\n",
    "    if revenue_per_city:\n",
    "        sorted_revenue_per_city = sorted(revenue_per_city.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        print(f\"{'City/Country':<30} | {'Total Revenue':>15}\")\n",
    "        print(\"-\" * 48)\n",
    "        for city, total_revenue in sorted_revenue_per_city:\n",
    "            print(f\"{city:<30} | {total_revenue:>15.2f}\")\n",
    "    else:\n",
    "        print(\"Could not calculate revenue per city. Aggregation dictionary is empty.\")\n",
    "else:\n",
    "    print(\"Transaction list (all_transactions) not loaded or empty. Please run previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5a530",
   "metadata": {},
   "source": [
    "\n",
    "11\tSerialization Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7aad827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Serializing Cleaned and Enriched Transaction Data ---\n",
      "Successfully saved cleaned data to JSON: data/cleaned_transactions.json\n",
      "Successfully saved cleaned data to CSV: data/cleaned_transactions.csv\n",
      "Successfully saved cleaned data to Parquet: data/cleaned_transactions.parquet\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "if 'all_transactions' in globals() and all_transactions:\n",
    "    print(\"--- Serializing Cleaned and Enriched Transaction Data ---\")\n",
    "    transactions_dict_list = []\n",
    "    for t in all_transactions:\n",
    "        transactions_dict_list.append({\n",
    "            'date': t.date.strftime('%Y-%m-%d %H:%M:%S') if t.date else None,\n",
    "            'customer_id': t.customer_id,\n",
    "            'product': t.product,\n",
    "            'price': t.price,\n",
    "            'quantity': t.quantity,\n",
    "            'coupon_code': t.coupon_code,\n",
    "            'shipping_city': t.shipping_city,\n",
    "            'order_id_original': t.order_id_original,\n",
    "            'region_original': t.region_original,\n",
    "            'sales_channel_original': t.sales_channel_original,\n",
    "            'unit_cost_original': t.unit_cost_original,\n",
    "            'total_revenue_csv': t.total_revenue_csv, \n",
    "            'discount_rate': t.discount_rate,\n",
    "            'discount_amount': t.discount_amount,\n",
    "            'subtotal': t.subtotal,                 \n",
    "            'final_total': t.final_total,           \n",
    "            'days_since_purchase': t.days_since_purchase\n",
    "        })\n",
    "\n",
    "    df_cleaned_transactions = pd.DataFrame(transactions_dict_list)\n",
    "\n",
    "    json_output_path = 'data/cleaned_transactions.json'\n",
    "    parquet_output_path = 'data/cleaned_transactions.parquet'\n",
    "    csv_output_path = 'data/cleaned_transactions.csv'\n",
    "\n",
    "    try:\n",
    "        df_cleaned_transactions.to_json(json_output_path, orient='records', lines=True, date_format='iso', default_handler=str)\n",
    "        print(f\"Successfully saved cleaned data to JSON: {json_output_path}\")\n",
    "\n",
    "        df_cleaned_transactions.to_csv(csv_output_path, index=False)\n",
    "        print(f\"Successfully saved cleaned data to CSV: {csv_output_path}\")\n",
    "        \n",
    "        try:\n",
    "            df_cleaned_transactions.to_parquet(parquet_output_path, index=False, engine='pyarrow') \n",
    "            print(f\"Successfully saved cleaned data to Parquet: {parquet_output_path}\")\n",
    "        except ImportError:\n",
    "            print(\"\\nError: 'pyarrow' library is required to save to Parquet. Parquet file not saved.\")\n",
    "            print(\"Please install it: pip install pyarrow (or conda install pyarrow if using Conda)\")\n",
    "        except Exception as e_parquet:\n",
    "            print(f\"\\nAn error occurred during Parquet serialization: {e_parquet}. Parquet file may not be saved correctly.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during general serialization: {e}\")\n",
    "else:\n",
    "    print(\"Transaction list (all_transactions) not loaded or is empty. Please run previous steps to generate data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d25e2",
   "metadata": {},
   "source": [
    "12\tSoft Interview Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf5a5f8",
   "metadata": {},
   "source": [
    "Object-Oriented Programming (OOP) found full expression here, employing the Transaction class to bundle transaction data (price, product) with its manufactures or operations (clean(), apply_discount(), calculate_final_total()). By keeping the data and methods organized and logically grouped, cleaning, transformation, and feature engineering could all be neatly implemented with shared code applied equally to every record. For instance, discounting logic or cleaning rules that were complex would reside within those class methods, becoming easier to read and maintain. Such a structured way of working is a great way of managing complex data pipelines and developing in teams, much of the real-world engineering data are legible and reusable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1b37e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d5ef8",
   "metadata": {
    "vscode": {
     "languageId": "csv"
    }
   },
   "outputs": [],
   "source": [
    "## Data Dictionary\n",
    "\n",
    "\n",
    "| Field Name                    | Type (Python/Pandas)         | Description                                                                 | Source                           |\n",
    "|-------------------------------|------------------------------|-----------------------------------------------------------------------------|-----------------------------------|\n",
    "| **Order Date (original)**     | object / string              | The original date string when the order was placed.                         | Primary CSV                      |\n",
    "| **Order ID (original)**       | int64 / string               | Unique identifier for the order from the source CSV.                        | Primary CSV                      |\n",
    "| **Item Type (original)**      | object / string              | The original type or category of the item sold.                             | Primary CSV                      |\n",
    "| **Units Sold (original)**     | int64                        | The number of units of the item sold in the transaction, from source.       | Primary CSV                      |\n",
    "| **Unit Price (original)**     | float64                      | The price of a single unit of the item, from source.                        | Primary CSV                      |\n",
    "| **Unit Cost (original CSV)**  | float64                      | The cost of a single unit of the item to the seller, from source.           | Primary CSV                      |\n",
    "| **Total Revenue (original CSV)** | float64                   | The total revenue recorded in the original CSV for the transaction.          | Primary CSV                      |\n",
    "| **Sales Channel (original)**  | object / string              | The channel through which the sale was made (e.g., Online, Offline).        | Primary CSV                      |\n",
    "| **Country (original)**        | object / string              | The country associated with the transaction from source.                    | Primary CSV                      |\n",
    "| **Region (original CSV)**     | object / string              | The region associated with the transaction from source.                     | Primary CSV                      |\n",
    "| **city (worldcities)**        | string / object              | Name of the city from the secondary dataset.                                | worldcities.csv                  |\n",
    "| **country (worldcities)**     | string / object              | Name of the country the city belongs to, from the secondary dataset.        | worldcities.csv                  |\n",
    "| **population (worldcities)**  | float64                      | Population of the city (used if selecting most populous).                   | worldcities.csv                  |\n",
    "| **capital (worldcities)**     | string / object              | Type of capital (e.g. primary, admin) for the city.                         | worldcities.csv                  |\n",
    "| **date**                      | datetime                     | Processed order date as a Python datetime object.                           | Engineered from 'Order Date'     |\n",
    "| **customer_id**               | string                       | Customer identifier, proxied by 'Order ID'.                                 | Engineered from 'Order ID'       |\n",
    "| **product**                   | string                       | Cleaned and standardized item type (e.g., title cased, stripped whitespace).| Engineered from 'Item Type'      |\n",
    "| **price**                     | float                        | Cleaned unit price as a float (e.g., non-negative).                         | Engineered from 'Unit Price'     |\n",
    "| **quantity**                  | int                          | Cleaned units sold as an integer (e.g., non-negative).                      | Engineered from 'Units Sold'     |\n",
    "| **coupon_code**               | string / None                | Synthetically generated coupon code applied to the transaction.             | Engineered (Synthetic)           |\n",
    "| **shipping_city**             | string                       | Derived shipping city, based on 'Country' (from sales) and worldcities.csv. | Engineered                       |\n",
    "| **order_id_original**         | string                       | Stored original Order ID from the sales data.                               | From Primary CSV via Transaction |\n",
    "| **region_original**           | string / None                | Stored original Region from the sales data.                                 | From Primary CSV via Transaction |\n",
    "| **sales_channel_original**    | string / None                | Stored original Sales Channel from the sales data.                          | From Primary CSV via Transaction |\n",
    "| **unit_cost_original**        | float                        | Stored original Unit Cost from the sales data (defaulted to 0.0 if missing/invalid). | From Primary CSV via Transaction |\n",
    "| **total_revenue_csv**         | float                        | Stored original Total Revenue from the sales data (defaulted to 0.0 if missing/invalid). | From Primary CSV via Transaction |\n",
    "| **discount_rate**             | float                        | Numeric discount rate (0.0 to 1.0) derived from coupon_code.                | Engineered from coupon_code      |\n",
    "| **discount_amount**           | float                        | Calculated monetary value of the discount (subtotal * discount_rate).       | Engineered                       |\n",
    "| **subtotal**                  | float                        | Calculated price * quantity (using cleaned price/quantity) before discount. | Engineered                       |\n",
    "| **final_total**               | float                        | Calculated final revenue after applying discounts (subtotal - discount_amount). | Engineered                   |\n",
    "| **days_since_purchase**       | int / None                   | Number of days from the transaction date to the latest date in the dataset.  | Engineered from date             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d914a56",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://www.kaggle.com/datasets/viswanathanc/world-cities-datasets\n",
    "2. https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea5d40",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
